{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/Project/Environments/env_GenAI_Chatbot/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "os.environ[\"NOMIC_API_KEY\"] = os.getenv(\"NOMIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks(text,ids,file_name):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=50000, chunk_overlap=1000)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    ids_current = [f\"{file_name}.{l}\" for l in range(len(chunks))]\n",
    "    ids+=ids_current\n",
    "    return chunks,ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_docs,folder_path):\n",
    "    ids=[]\n",
    "    chunks_all=[]\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader= PdfReader(os.path.join(folder_path,pdf))\n",
    "        print(f\"Processing file - {pdf}\")\n",
    "        text=\"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text+= page.extract_text()\n",
    "        chunks,ids = get_text_chunks(text,ids,pdf)\n",
    "        chunks_all+=chunks\n",
    "    return  text,chunks_all,ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Googel/Nomic API key -----\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "\n",
    "embeddings_google = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "embeddings_nomic = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(embeddings_api,text_chunks,ids):\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings_api,ids=ids)\n",
    "    #vector_store = FAISS.from_documents(documents=text_chunks , embedding=embeddings_api)\n",
    "    \n",
    "    #vector_store.save_local(\"faiss_index\")\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_chain():\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
    "\n",
    "    prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])\n",
    "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
    "\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file - AGI.pdf\n",
      "Processing file - Understanding LLMs.pdf\n",
      "Processing file - ComputerVision.pdf\n",
      "Processing file - AI Assistants.pdf\n",
      "Processing file - Augmented LLMs.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLM A UGMENTED LLM S:\\nEXPANDING CAPABILITIES THROUGH COMPOSITION\\nRachit Bansal1Bidisha Samanta1Siddharth Dalmia2Nitish Gupta1Shikhar Vashishth1\\nSriram Ganapathy1Abhishek Bapna1Prateek Jain1Partha Talukdar1\\n1Google Research2Google DeepMind\\nABSTRACT\\nFoundational models with billions of parameters which have been trained on large\\ncorpora of data have demonstrated non-trivial skills in a variety of domains. How-\\never, due to their monolithic structure, it is challenging and expensive to augment\\nthem or impart new skills. On the other hand, due to their adaptation abilities,\\nseveral new instances of these models are being trained towards new domains and\\ntasks. In this work, we study the problem of efficient and practical composition\\nof existing foundation models with more specific models to enable newer capa-\\nbilities. To this end, we propose CALM—Composition to Augment Language\\nModels—which introduces cross-attention between models to compose their rep-\\nresentations and enable new capabilities. Salient features of CALM are: (i) Scales\\nup LLMs on new tasks by ‘re-using’ existing LLMs along with a few additional\\nparameters and data, (ii) Existing model weights are kept intact, and hence pre-\\nserves existing capabilities, and (iii) Applies to diverse domains and settings. We\\nillustrate that augmenting PaLM2-S with a smaller model trained on low-resource\\nlanguages results in an absolute improvement of up to 13% on tasks like trans-\\nlation into English and arithmetic reasoning for low-resource languages. Simi-\\nlarly, when PaLM2-S is augmented with a code-specific model, we see a relative\\nimprovement of 40% over the base model for code generation and explanation\\ntasks—on-par with fully fine-tuned counterparts.\\n1 I NTRODUCTION\\nLarge Language Models (LLMs) have shown to encompass a range of foundational capabilities\\nsuch as commonsense and factual reasoning, world knowledge, and coherent language generation\\n(Bubeck et al., 2023; Google et al., 2023). Leveraging these foundational capabilities, a number of\\nefforts in the community have fine-tuned these models to enable domain-specific capabilities such as\\ncode generation, copy editing, and mathematical problem solving (Lewkowycz et al., 2022; Singhal\\net al., 2023). This has resulted in the development of several specialized large models with domain-\\nspecific capabilities. For example, there are models that do well on standard code generation but\\nare not as proficient in general logical reasoning and vice-versa. Presence of such a large number\\nof domain-specific models leads to a natural question: Can we compose an anchor model with\\na domain-specific augmenting model to enable new capabilities? For example, can we compose\\nan augmenting model’s code understanding capability with an anchor LLM’s language generation\\ncapability to enable code-to-text generation capability?\\nThe typical approach for this problem is to further pre-train or (efficiently) fine-tune the anchor\\nmodel on the data that was originally used to train the augmenting model (Hu et al., 2022; Kessler\\net al., 2021). However, many a times such solutions are not feasible since training large models is\\ncomputationally expensive, especially since the augmenting model itself may be an LLM trained\\non a massive corpora. Further, processing data from multiple sources might not be feasible due\\nto privacy concerns and organizational boundaries. Working with multiple distinct models is also\\ndesirable since it allows the reuse of existing models with established capabilities, providing better\\ncontrol and avoiding catastrophic forgetting that is prevalent in conventional approaches.\\nCorrespondence to Rachit and Bidisha: [brachit, bidishasamanta]@google.com\\n1arXiv:2401.02412v1  [cs.LG]  4 Jan 2024mBmA\\nTranslate from XX to En:\\n<Source XX Sentence>Everything but the kitchen sink\\nLow -resource\\nLanguage \\nPre-trained\\nWhat does this Python code do?\\n<Python Code Snippet>Implements the classic word \\ngame of Hangman\\nm BKey-value \\nMapping\\nx1 = 10\\n     \\nxn = 2x1 = 10\\n     \\nxn = 2\\nm A\\nlA,ilA,i lB,jlB,(j+1)\\nWKWVWQAttention\\nWhat is the value of x 1 + x 8 * xn?Since x 1=10, x 8=14, x n=2, x 1 + x 8 * xn = 38\\nmBmAPre-trained \\non GitHubNumeric Arithmetic\\nFigure 1: Overview of CALM. To augment an anchor LLM ( mB) with new capabilities through\\ncomposition with a specialized augmenting model ( mA). Figure illustrates three mAwith differ-\\nent capabilities: key-value mapping ( left), low-resource languages ( center ), and code ( right ). Mod-\\nelsmAandmBremain unchanged ( ^) during composition. A few additional parameters are learnt\\nover models’ layer representations. Leftmost plot shows an mAtrained on a set of string-integer\\nmappings, e.g., {x1: 10,. . .,xn: 2}.mBis a large LM with arithmetic capabilities. CALM com-\\nposes these two frozen models to solve the task of arithmetic on keys which either models could not\\nsolve on their own (§4.1). Notably, CALM generalizes to the entire key-value set despite training\\nwith arithmetic examples spanning only 20% of the keys.\\nTo address the training and the data challenges mentioned above, we propose and study a practical\\nsetting for model composition : (i) we are given access to one (or more) augmenting model(s) and an\\nanchor model, (ii) we are not allowed to modify the weights of either models, and (iii) we only have\\naccess to a small amount of data, representing the “combined skills” of the given models, e.g., code\\ngeneration with complex logical reasoning.\\nPrior work has largely approached the question of composition from either a routing or a merging\\nstandpoint, neither of which provide an effective solution to capture this setting. Routing between the\\ngiven models, i.e., choosing an output of one model over the other (Ma et al., 2019), or performing a\\nsoft ensemble (Muqeeth et al., 2023) is not effective when neither of the models can demonstrate the\\ndesired capability. Another body of work creates a combined model by an arithmetic combination\\nof base model parameters (Wortsman et al., 2022; Ilharco et al., 2022; Matena & Raffel, 2022).\\nHowever, these settings are naturally restrictive and their efficacy is unclear when combining models\\nwith different sizes and pre-training objectives (Yadav et al., 2023).\\nIn this work, we propose a novel Composition to Augment Language Models (CALM ) framework\\nto address the general model composition setting mentioned above. Rather than a shallow combi-\\nnation of the augmenting and anchor LMs (Wortsman et al., 2022; Ilharco et al., 2022), CALM\\nintroduces a small number of trainable parameters over both augmenting and anchor models’ inter-\\nmediate layer representations. CALM finds an effective combination of the given models to perform\\nnew challenging tasks more accurately than either of the models alone, while preserving the capa-\\nbilities of individual models. Figure 1 highlights few motivating scenarios for CALM.\\nWe study key practical applications of CALM: language inclusivity and code generation. For lan-\\nguage inclusivity (§4.2), we use a model that has been trained on a set of low-resource languages.\\nWe observe that composing this model with the LLM allows us to borrow its generation and reason-\\ning capabilities to achieve significantly better performance on translation and arithmetic reasoning\\ntasks for low-resource languages (Tables 2 and 3). This composed model outperforms not only the\\ntwo base models but also versions of the LLM that have been further pre-trained or LoRA (Hu et al.,\\n2022) fine-tuned for the set of low-resource languages. For code generation (§4.3), we use a model\\nthat has been trained on open-source code across a variety of programming languages. Compos-\\ning this model with the LLM—hence borrowing its low-level logic and generation capabilities—\\noutperforms the two base models (Table 4) on code explanation and code completion tasks.\\n22 R ELATED WORKS\\nParameter efficient fine-tuning: A large body of work focuses on efficient ways of fine-tuning\\nmodels for new domains by introducing a small number of trainable parameters, keeping the original\\nmodel intact (Houlsby et al., 2019; Wang et al., 2021; Pfeiffer et al., 2021; Hu et al., 2022; Kessler\\net al., 2021). Since this paradigm allows a small set of new parameters to be trained, it is challenging\\nto use this approach to adapt a model to a new domain, which is absent from the original training\\ncorpus. In contrast, CALM enables a model to be adapted to completely new domains using an\\naugmenting model. In Section 4.4, we demonstrate that CALM is significantly more effective than\\nLoRA (Hu et al., 2022), a representative parameter efficient fine-tuning method.\\nModel Merging: Merging different expert models with simple techniques like task vector aver-\\naging provides a way of recombining different capabilities of these models (Ilharco et al., 2022;\\nMatena & Raffel, 2022). However, these methods are only relevant when the original models are\\nwell aligned. Other related approaches are also applicable only when the models are derived from\\nthe same model (Matena & Raffel, 2022) or they are of same size (Muqeeth et al., 2023). In contrast,\\nCALM is more generic and is applicable to any set of models.\\nModel and Task Compositionality: The modular encoder-decoder based method in (Dalmia\\net al., 2022) adapts components of encoder-decoder models to allow flexible re-usability of dif-\\nferent encoders, each with their own capabilities. Several past studies explore compositionality\\nfrom a multi-modal standpoint. Alayrac et al. (2022) introduce cross-attention parameters across a\\nlanguage model in order to attend to representations coming from an image encoder. They show\\nvery effective transfer of capabilities between the two models. In this work, we extend the ideology\\nof model re-use and modularity to extend composition of capabilities in a large language model.\\nModels as Tools: Another interesting direction for using multiple language models to solve a\\ndownstream task has been to perform composition in the models’ input text space (Zeng et al.,\\n2022; Shen et al., 2023). Schick et al. (2023) have demonstrated how a model can be taught to use\\nexternal tools—there might be an opportunity to investigate if other models can be called as a part\\nof the same framework. Since these approaches require a large amount of prompt engineering, in\\nthis work we focus on composition through representations that can be learnt automatically.\\n3 C OMPOSITION TO AUGMENT LANGUAGE MODELS (CALM)\\nGiven an anchor model mBand an augmenting model mA, CALM aims to compose the two models\\n(mA⊕B) to enable new capabilities as a composition of capabilities of the two individual models.\\nAs discussed in the introduction, we study this composition in a practical setting with the following\\nassumptions: i) we can access weights, run forward and backward pass, and access intermediate\\nrepresentations of both mBandmA, ii) we are not allowed to change weights of both the models,\\niii) we do not have access to the training data, hyperparameters, training states of both the base\\nmodels, iv) we are provided a few examples from the target composition domain.\\nThe goal is to learn a composition mA⊕B=f(mA,mB,ΘC,DC) to achieve some joint task C. The\\nweights of mAandmBare frozen. ΘCis the additional set of trainable parameters introduced to\\nlearn the composition and DCrefers to the set of examples that are used to learn this composition.\\n3.1 L EARNING TO COMPOSE (ΘC)\\nAs outlined in Figure 1, we operate over a selected set of layers from mBandmAat all times. We\\nlearn two sets of additional parameters over these layers: (i) A simple set of linear transformations,\\nfproj(.) that maps an ithlayer representation from mAto the dimensionality of representations from\\nmB, and (ii) A set of cross-attention layers, fcross(.,.) that cross-attend between this transformed\\nlayer representation and a jthlayer representation from mB.\\nCompositional Layers: Let the augmenting model mAand the anchor model mBhaveNAand\\nNBlayers, respectively. Also, let DAandDBbe the token dimensionality of the two models. We\\nfirst choose a set of compositional layers— LAandLB—for both models, over which the set of new\\n3learnable parameters are introduced during composition. nA=|LA|andnB=|LB|. For simplicity,\\nwe set nA=nB=nand the gap between two contiguous selected layers is kept uniform based\\non the number of selected layers—that is, ( l2−l1) =···= (ln−l(n−1)) =N/n . Further, HA\\n∈ {HA1,HA2, . . . ,HAnA}denote the layer representation of a given input after each layer in LA.\\nLearned Projections: Next we map representations from mAto that of mBvia a projection layer.\\nIn particular, for each layer in LA, we learn a projection function fproj:RDA→RDB, that projects\\nrepresentations from these layers to the desired representation size of mB. Let,\\nfproj(HA)← − {fproj(HA1), fproj(HA2), . . . , f proj(HAnA)}\\nThis transformation enables cross-attention across models, and also performs an alignment of rep-\\nresentations from mAandmBdespite frozen weights of the base models.\\nCross-attention Layers: Similar to the multi-headed cross-attention in encoder-decoder models\\n(for example Vaswani et al. (2017) and Raffel et al. (2020))—we introduce cross-attention between\\nrepresentations of the anchor and the augmenting model. In particular, we use fproj(HAi)from the\\naugmenting model as the keyandvalue vectors for each head in cross-attention. We use the vector\\nHBjfrom the anchor model as the query vector, which leads to the following cross-attention setup:\\nfcross(fproj(HAi),HBj) =Concat. k(head k)WO∀k∈NH\\nwhere, head k=Attn.(QB,KA,VA),\\nand,QB=HBjWQ\\nk,\\nKA,VA=fproj(HAi)WK\\nk, fproj(HAi)WV\\nk\\nHere, NHrepresents the number of attention heads used for cross-attention which, in our case, is\\ntypically the same as the number of heads used for self-attention in mB. Each of WO∈RDB×DB,\\nWQ\\nk,WK\\nk, andWV\\nk∈RDB×DB//NHare learnable weight matrices, where k∈ {1..NH}.\\nFinally, the cross-attention output is added as a residual connection to the layer representations of\\nmB. The resultant output vector, in-turn, is the input to the succeeding layer in mB:\\nHA⊕Bj=HBj+fcross(fproj(HAi),HBj)\\nHere,HA⊕Bjdenotes the input to the (j+ 1)thlayer of the composed model. All layers in LAand\\nLBare utilized in a similar manner. Propagating over the remaining layers in mBgives us a final\\noutput token ytdecoded for the tthtimestep. Akin to usual auto-regressive decoding, the output\\ntoken for each time-step is appended to the input: xt+1=xt⊕yt, Since the updated input at each\\ntime step is passed to both models, all representations for the two models are refreshed.\\n3.2 C OMPOSITION TRAINING DATA (DC)\\nSince the target model mA⊕Binvolves a composition over the two models mAandmB, we construct\\nthe set of training examples DCto depict a “combined skill” that enables ΘCto attend over the two\\nmodels appropriately for the target task.\\nIdeally, if the set of tasks involved in composition task are distinguished as t1andt2respectively,\\nthen we design DCto depict the a joint task C. For example, with respect to our synthetic key-value\\nsetup: our final task ( C) is to perform arithmetic over a set of keys. The augmenting model mAis\\ntrained to learn the given key-value pairs (notated as task, t1) and the anchor model mBis generic\\nmodel that can perform numeric arithmetic well (task t2). For learning the set of parameters ΘCfor\\ncomposition, we consider DCto be arithmetic over a held-in set of keys (task C), encompassing\\ncombined skills from the two models. In contrast to fine-tuning approaches like LoRA (Hu et al.,\\n2022) that would require the entire knowledge source (here, key-values) during training time, we\\nfind that training composition on only a fraction of the keys can generalize to the full set.\\nIn other real world settings, a clear distinction in specializing tasks for each model might be difficult\\nto formulate and hence defining a task that captures the combined skills can be challenging. We find\\nthat using a set of examples that capture certain capabilities of the two models suffices, i.e., some\\nrough notion of tA∪B. For our language inclusivity task, we use a mixture of examples containing\\na small amount of low-resource language and high-resource language data.\\n4Composing multiple models: Finally, we note that while the method has been presented for a\\nsetting with one anchor model and only one augmenting model, CALM is applicable to multiple\\naugmenting models as well. In particular, CALM would require learning similar projection and\\ncross-attention components between the anchor and each of the augmenting model. We leave a\\nthorough investigation of this as a topic of future work.\\n4 E XPERIMENTS\\nWe demonstrate the following in three domains: (a)an anchor LLM ( mB) can be composed with an\\naugmenting model ( mA) trained on mappings between string keys and number values to solve arith-\\nmetic expressions over those keys requiring both, knowledge of the KV mappings and arithmetic\\ncapabilities (§4.1); (b)how CALM can be used to expand the language coverage of an anchor LLM\\n(mB) to low-resource languages it has not seen during pre-training. We show that an augmenting\\nmodel ( mA) pre-trained on low-resource languages can be composed with such an anchor model to\\nsignificantly improve translation and math-word problem solving capabilities in low-resource lan-\\nguages (§4.2); (c)how code completion and explanation can be improved by composing an anchor\\nLLM with an augmenting model ( mA) specializing in the code domain (§4.3).\\nIn all experiments, we start with a PaLM2-XXS model and further train it on domain-specific data to\\narrive at an augmenting model ( mA) that is then kept frozen during composition. Note that no task\\nspecific training data was used to train CALM. We use PaLM2-XS or PaLM2-S models as the anchor\\nLLM ( mB) that is also kept frozen during composition training. For all our experiments, we set\\nNA/n= 4, i.e., we perform composition using every 4th layer output from mA. Correspondingly,\\nlayers from mA(LB) are chosen such that nB=nA=n, hence nB=NA/4.\\n4.1 K EY-VALUE ARITHMETIC\\nWe first study the setting where we have a small augmenting LM that has been trained to memorize\\nstring-to-integer key-value (KV) mappings, and a large anchor LM that is capable of performing\\narithmetic over integers. We wish to use CALM to compose them and enable a new capability of\\nsolving arithmetic expressions containing those keys.\\nKey-Value Domain Knowledge We first generate a repository of KV pairs containing N KV= 25 K\\npairs by sampling English strings of length 2−6characters from the vocabulary of the PaLM2-XXS\\nmodel and randomly assigning them unique integer values in the range [1,NKV]. This constitutes\\nthe knowledge artifact, DKV. We further generate a collection of arithmetic expressions ( DKV-EXP )\\ncontaining addition ( +), subtraction ( −), and multiplication ( ×) operations between 3−6keys by\\nrandomly sampling keys from DKVand operations to perform between them.\\nUsing these arithmetic expressions, we generate three datasets:\\n(i)KV-Substitution ( DKV-SUBS ): This dataset maps each expression in DKV-EXP , to an expression\\nwhere the keys are replaced by their corresponding values. For example, this dataset contains exam-\\nples of the form ( <K1> +<K2>−<K3> ,10 + 22 −24).\\n(ii)KV-Arithmetic ( DKV-MATH ): This dataset maps each expression in DKV-EXP to the numeric value\\narrived at by solving the arithmetic expression when the keys would be replaced by the correspond-\\ning values. For example, examples in this dataset look like ( <K1> +<K2>−<K3> ,8).\\n(iii) Numeric-Arithmetic ( DNUM-MATH ): This dataset maps the value substituted version of each\\nexpression in DKV-EXP to the numeric value arrived at by solving the arithmetic expression. For\\nexample, examples in this dataset look like ( 10 + 22 −24,8).\\nModels We obtain augmenting model mAby further training a pre-trained PaLM2-XXS model on\\nDKV-SUBS to make it memorize the KV pairs in DKV. Note that, training on DKV-SUBS does not teach\\nthis augmenting model how to solve arithmetic expressions. Next, we use a pre-trained PaLM2-XS\\nmodel as the anchor model mB. This model is capable of solving numeric expressions with decent\\nperformance (see Table 1). Note that, this model has no knowledge of the KV pairs in DKV.\\nWe now take examples from the KV-Substitution dataset DKV-SUBS that only span 20% of the keys in\\nDKVto form the training data for composition ( DC). We use DCto compose the augmenting model\\n5(mA) having knowledge of DKVand the pre-trained anchor model mBby training the composition\\nparameters ( ΘC) using CALM as explained in §3. Both mAandmBare kept unchanged.\\nEvaluation Task We evaluate the composed model mA⊕Bfor its ability to solve arithmetic ex-\\npressions containing keys from DKV. Specifically, we evaluate on the subset of DKV-MATH dataset\\nthat does not contain expressions used in DCduring training. This way, we are able to measure the\\ncomposed model’s ability to generalize to keys beyond what was observed during training.\\nmAmBCALM\\n(mA⊕B)\\nDKV-SUBS 98.1 0.0 92.9\\nDNUM-MATH 4.2 73.7 72.0\\nDKV-MATH 0.7 0.0 84.3\\nTable 1: Evaluation (accuracy (%)) for\\na synthetic key-value (KV) task. mA\\nis trained to memorize the KV mappings\\nwhile mBexcels at arithmetic We see that\\na composition mA⊕Bis able to perform\\narithmetic over held-out keys.Results Table 1 shows the performance of the three\\nmodels: mA,mB, and mA⊕Bacross the aforemen-\\ntioned datasets. First, we observe that the augmenting\\nmodel mAachieves 98.1%at the KV-Substitution task\\nshowing that memorizes DKVwell. Next, we see that\\nit performs poorly ( 4.2%) at the Numeric-Arithmetic\\ntask showing that it does not have arithmetic capabili-\\nties. As a result, this model is not able to solve arith-\\nmetic expressions containing keys from DKV.\\nAs expected, the anchor model mBgets0%accuracy\\non the KV-Substitution and KV-Arithmetic tasks as it\\nhas not seen any data from DKV. However, it performs\\nwell ( 73.7%) on the Numeric-Arithmetic task demon-\\nstrating capability of arithmetic over numerals.\\nLastly, we see that the composed model mA⊕Bis able\\nto solve all tasks with high accuracy, especially the KV-Arithmetic task ( 84.3%) which both the\\nunderlying models fail at. This shows that the composed model is able to leverage the relevant\\ncapabilities from both the augmenting and anchor model to solve a complex task.\\n4.2 L OW-RESOURCE LANGUAGE INCLUSIVITY\\nFLORES-200 (XX to En; chrF1)\\nModellij mr taq nn su ban pl th min acm avg.\\nPaLM2-XXS 24.0 16.5 21.6 33.3 20.6 2.1 5.3 63.2 44.0 59.8 29.0\\n+NTL ( mA) 32.0 21.6 46.9 50.0 40.6 4.1 4.0 63.8 47.8 61.1 37.2\\nPaLM2-S ( mB) 32.6 24.2 44.6 50.8 50.9 5.4 9.5 69.0 61.0 68.6 41.7\\nCALM ( mA⊕B) 44.1 30.4 55.1 54.6 54.4 11.8 11.3 69.4 61.1 68.9 46.1\\nmB+NTL ( mNTL\\nB)48.1 39.1 59.2 57.5 57.3 11.4 9.9 69.4 61.4 69.0 48.2\\nTable 2: Translation performance for XX to English direction on the FLORES-200 dataset (Costa-\\njuss`a et al., 2022): We show results for a subset of 10 low-resource languages. Note that the com-\\nposed model mA⊕Bsignificantly outperforms both mAandmB. On the complete language list,\\nmA⊕Boutperforms both the underlying models for 175 of 192 languages (Appendix A; Figure 2).\\nmNTL\\nBrepresents a skyline where mBhas been further pre-trained on DNTL. The composed model\\nachieves similar performance for a tiny fraction of the training cost.\\nIn this section, we study if we can compose such a large anchor LM mBwith a smaller augmenting\\nLMmAthat has been pre-trained on low-resource languages, to perform translation and math-word\\nproblem solving tasks presented in these low-resource languages.\\nLow-resource Language Corpora We use the long-tail language set and the associated corpora\\nfrom the Next Thousand Languages (NTL) effort (Caswell et al., 2020; Bapna et al., 2022) as the\\ndomain data DNTL. This large-scale corpora contains web-crawled monolingual sentences and trans-\\nlation pairs for ∼1000 languages. The dataset has been used for language expansion in translation\\nsystems and language models (Garcia et al., 2021; Siddhant et al., 2022).\\n6GSM8K (Low-resource Languages; Accuracy)\\nModelmeo mfa pcm efi min ilo ady mai nso mzn avg.\\nPaLM2-XXS 5.2 6.8 6.8 4.0 5.6 7.2 6.0 3.6 7.2 6.8 5.9\\n+NTL ( mA) 7.6 4.0 4.4 3.2 6.0 4.8 6.4 3.2 6.0 4.8 5.0\\nPaLM2-S ( mB)28.8 14.0 34.4 14.8 25.2 14.8 30.0 22.8 8.4 31.6 22.5\\nCALM ( mA⊕B)34.0 17.6 33.6 18.0 23.6 16.8 36.4 24.8 8.4 36.4 25.0\\nmNTL\\nB 33.2 20.4 31.6 14.0 24.8 14.0 29.2 21.2 9.6 27.6 22.6\\n(High-resource Languages)\\nModelen te bn sw ja zh th fr es de avg.\\nPaLM2-XXS 5.6 4.0 2.0 7.6 2.0 4.4 6.0 6.8 5.6 9.2 5.3\\n+NTL ( mA) 4.8 3.6 3.2 4.8 3.2 7.6 6.4 9.2 5.6 7.2 5.6\\nPaLM2-S ( mB)36.8 19.2 23.2 16.0 2.0 39.2 29.6 38.0 32.4 43.2 28.0\\nCALM ( mA⊕B)37.2 28.0 27.2 18.0 2.4 43.6 33.2 42.8 36.0 49.2 31.8\\nmNTL\\nB 36.0 17.6 18.4 14.4 0.8 33.6 27.2 34.8 31.2 42.0 25.6\\nTable 3: Evaluations for grade-school mathematics (GSM) problems on low-resource (LRL) and\\nhigh-resource (HRL) languages. We observe that CALM yields significant gains for both evaluation\\nsets. Gains on the HRL set suggests that CALM avoids catastrophic forgetting.\\nModels Akin to §4.1, we obtain augmenting model mAby training the PaLM2-XXS model on\\nDNTLto impart knowledge about these low-resource languages to the model. For mB, we use the\\npre-trained PaLM2-S model. We use ∼5%of the same low-resource language corpora DNTLas\\nthe training data DCto compose mAandmBvia CALM. Since both models are untrained during\\ncomposition, the anchor model mBisnottrained on any of the low-resource language data.\\nEvaluation Tasks We evaluate the composed model mA⊕Bon two tasks:\\n(i)Translating text from a non-English language to English: We carry out these evaluations in a\\n5-shot in-context learning paradigm on the FLORES-200 (Costa-juss `a et al., 2022) dataset. This\\ndataset contains examples for 200 high- and low-resource languages.\\n(ii)Performing grade school math word problems expressed in a non-English language: We evaluate\\non the multilingual version of the GSM-8K dataset (Shi et al., 2023) containing math word problems\\nfor English and 9 other high-resource languages. We further generated a silver-standard GSM-8K\\ndataset for low-resource languages by automatically translating the English examples in GSM-8K\\nto 25 low-resource languages supported by Google Translate.1\\nResults Table 2 shows results on the FLORES-200 dataset (Costa-juss `a et al., 2022), where the\\ninput is a low-resource (XX) language sentence and the output should be the corresponding English\\ntranslation. For 10 low-resource languages shown in the Table, we see that both the underlying\\nmodels mAandmBare outperformed by our composed model mA⊕B. We find that the composed\\nmodel mA⊕Boutperforms mBon 175 of the complete set of 192 languages (Appendix A).\\nTable 3 shows the performance of these models on the grade-school math word problems from the\\nGSM8K task (Cobbe et al., 2021) on low-resource languages ( top) and high-resource languages (Shi\\net al. (2023); bottom ). Firstly, we observe that the augmenting model mAdoes not perform well on\\nthis task due to its limited mathematical reasoning capabilities. On the other hand, the anchor model\\nmBdoes much better given its mathematical reasoning capabilities and transfer-learning from high-\\nresource languages. Finally, we observe that mA⊕Boutperforms both mAandmBon18 of 25\\nlow-resource and 9 of 10 high-resource languages, demonstrating effective composition of models.\\nSee Table 6 (Appendix A.2) for a complete set of evaluations. Note that the last row in Table 3 shows\\nthatmBwhen fine-tuned on DNTLleads to worse performance than the pre-trained mBindicating\\nforgetting. Composing domain-specific model mAwithmBusing CALM avoids this.\\n1We perform quality evaluations in Appendix 7.\\n7Model CC (P@1) T2C (P@1) C2T (chrF1)\\nHumanEval MBPP Python PHP Go Java JS Ruby\\nPaLM2-XXS\\n+ Code ( mA)19.5 28.0 28.0 34.7 32.6 29.6 26.5 26.0\\nPaLM2-S ( mB) 16.4 28.6 30.4 35.5 40.4 31.0 28.8 27.9\\nCALM ( mA⊕B) 22.5 32.2 30.5 35.8 40.6 31.4 29.3 29.0\\nmCode\\nB 24.3 43.0 18.9 35.0 41.1 31.1 20.2 27.6\\nTable 4: Evaluations for code generation and understanding across three tasks: Code Completion\\n(CC), Text-to-Code (T2C), and Code-to-Text (C2T). Augmenting code understanding to mBusing\\nmAsignificantly improves performances across all datasets. mCode\\nBrepresents a skyline where mB\\nfurther pretrained on the DCode, which shows catastrophic forgetting of text generation task.\\n4.3 C ODE UNDERSTANDING AND GENERATION\\nCode understanding and generation require two distinct types of capabilities: ( a) knowledge of the\\nsyntax and semantics of code, and ( b) knowledge of the world that the code is manipulating. While\\nLLMs have a wealth of world knowledge, they could often lack the specific knowledge of code\\nsyntax due to a skewed representation of code data in their pretraining corpora. Conversely, small\\nmodels trained specifically on code data could exhibit a good understanding of code syntax, but they\\nmay lack broad world knowledge and reasoning. CALM can enable best of both worlds.\\nCode Domain Data Here, we use the code-specific corpus, DCode, consisting of open-source code\\nextracted from GitHub heads for a variety of programming languages to train mA.\\nModels Similar to §4.1, a version of the PaLM2-XXS model has been further pre-trained on DCode\\nis used as mA, while the base pre-trained PaLM2-S model acts as mB. We build mA⊕Bby training\\nCALM with only 7% of the same code data (data used for mA) to have a data parity.\\nEvaluation Tasks We evaluate the efficacy of CALM on three different tasks:\\n(i)Code-Completion (CC): Given an initial set of lines of a code, the model is prompted to complete\\nthe code snippet. Here the aim is to evaluate the model for code syntax. We perform zero-shot eval-\\nuations on HumanEval benchmark dataset (Chen et al., 2021) and report the Pass@1 (P@1) metric.\\n(ii)Text-to-Code (T2C): Given a textual context, the model is prompted to generate the correspond-\\ning code snippet. Here, the evaluation indicates language understanding and code generation capa-\\nbilities. We perform 3-shot inference on the MBPP dataset (Austin et al., 2021) and report P@1.\\n(iii)Code-to-Text (C2T): Given a code snippet, the goal is to generate a natural language explanation\\nof the code. This task evaluates code understanding and text generation. We perform 3-shot evalua-\\ntions on the CodeXGlue benchmark (Lu et al., 2021) and report chrF1 scores across languages.\\nResults Table 4 reports comparative performance for the individual models mAandmB, the com-\\nposed version mA⊕B, and a fine-tuned anchor baseline mCode\\nB. Firstly, evaluations on the HumanEval\\ndataset suggest that mAhas a superior understanding of code syntax as a result of its additional train-\\ning on DCode. While, due to the larger scale and general purpose pre-training of mB, it excels at\\ngeneral language understanding and hence performs better on the T2C and C2T tasks.\\nWhen employing CALM to compose the two models, we observe a clear transfer and composition\\nof capabilities through significant performance improvements: 6.1%and3.6%absolute gains over\\nmBon the CC and T2C tasks, respectively. We observe that fine-tuning mBonDCodeleads to a\\nsignificant decline in the C2T performance due to catastrophic forgetting. CALM retains the perfor-\\nmance and is marginally better than mBacross all languages. We also study qualitative examples\\non the C2T task and observe interesting common patterns that are discussed in Appendix B.\\n8mNTL/Code\\nBCALM\\nmA⊕BVanilla\\nmARandom\\nmAmAas an\\nencoderLoRA\\nchrF1 62.1 60.5 59.2 58.8 59.3 59.2 FLORES-200\\n(XX-En) #( >mB) 171 175 115 43 102 82\\nAccuracy 19.8 21.4 19.0 17.8 19.1 20.9 GSM-8K\\n(LRL) #( >mB) 15 20 15 9 12 15\\nAccuracy 27.1 33.1 29.7 28.5 29.1 31.2 GSM-8K\\n(HRL) #( >mB) 1 11 8 4 6 9\\nHumanEval Pass@1 24.3 22.5 20.0 20.1 16.0 18.3\\nMBPP Pass@1 43.0 32.2 28.0 27.0 27.0 28.7\\nCodeXGLUE chrF1 29.0 32.6 32.2 32.1 32.0 32.6\\nTable 5: Comparative performance of CALM ( mA⊕B) across various possible ablations. The met-\\nric “#( >mB)” depicts the number of languages for which the corresponding model is better than\\nthe base for NTL, mB—out of 192, 25, and 11 languages for the three tasks respectively. For all\\ncompared settings, the number of added parameters are kept the same.\\n4.4 A BLATIONS\\nInfluence of mAWe first study the influence of mAby replacing it with vanilla and random\\nvariants during composition. Table 5 shows the variation of performance across NTL and Code tasks\\nwhen the specialized mAis replaced with a vanilla PaLM2-XXS checkpoint or an untrained version\\nof the model, i.e., a random model. We see that there is a considerable drop of performance with\\nthese variants across all tasks. On FLORES-200 XX-En task, languages improved with composition\\ndrop to 115 and 43 with vanilla and random, respectively. A slight improvement of the vanilla model\\novermBindicates that an un-specialized model (with a different training regime than mB) might\\nhave orthogonal capabilities leading to an enhanced model. This finding validates that performance\\ngains seen with CALM is a result of utilizing mAand not the added ΘCparameters.\\nInfluence of iterative decoding We also investigate a variation where we use mAas an encoder,\\ni.e., an output token decoded at a given timestep is not amended to mA’s input. In this case, only the\\nprefix representations of mAare used. This setting eludes to past work for image and text models\\n(Alayrac et al., 2022) where encoder and decoder models are composed. We observe a significant\\ndecline in performance across our various tasks when employing this setting.\\nComparision with LoRA Finally, we evaluate a parameter efficient fine-tuning approach by train-\\ning LoRA (Hu et al., 2022) layers to adapt mB. For all experiments, we set the LoRA rank such\\nthat the number of added parameters is equal to the number of parameters introduced with CALM.\\nWe also train LoRA on the same data as CALM, i.e., DC. We see a considerable difference in\\nperformance between the two approaches across all tasks and metrics.\\n5 C ONCLUSION\\nThe proposed CALM framework composes an anchor LLM with specialized augmenting models to\\nenable new tasks not achievable by either models individually. CALM does not require updating the\\nindividual models and learns a dense interaction between the models through a few trainable cross-\\nattention parameters. Our experiments present consistent evidence that CALM learns to utilize the\\nexpertise from the two models. That is, when composed with relevant augmenting models, we\\nobserve a significant uptick in the anchor model’s performance across multiple challenging tasks,\\nsuch as low-resource translation, reasoning, and code explanation/generation.\\nThat is, CALM is especially useful in scenarios where proprietary data and knowledge is stored in\\nparametric models. With CALM, a foundational LLM could be augmented with such proprietary\\nmodels to extend a variety of foundational capabilities such as reasoning, world knowledge, and\\ncoherent generation over the target proprietary domains. Finally, extensions of CALM could be\\nused to acquire distinct knowledge from multiple augmenting models.\\n9ACKNOWLEDGMENTS\\nThis work was done during RB’s pre-doctoral tenure at Google Research, India (GRI) with PT and\\nPJ. RB is indebted to Manish Gupta, Divvy Thakkar, and all others who enabled this oppurtunity.\\nRB would also like to thank the members of the Languages team and other researchers at GRI\\n(and beyond), including the incredible pre-doctoral cohort. This work wouldn’t have been possible\\nwithout their constant support. Namely: Aishwarya P.S., Laurent El Shafey, and Qiao Zhang for\\ntheir massive help in coding and debugging; Palak Jain and Sagar Gubbi for their feedback and\\nsupport throughout the project; Kartikeya Badola, Shreyas Havaldar, Amandeep Kaur, and Rishabh\\nTiwari for being the first ears to all ideas; Cyrus Rashtchian and Richa Dixit for their mentorship.\\nREFERENCES\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\\nBorgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\\nBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a Visual Language\\nModel for Few-Shot Learning, 2022. URL https://arxiv.org/abs/2204.14198 .\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\\nmodels. ArXiv preprint , abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.\\n07732 .\\nAnkur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Meng-\\nmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod,\\nJason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexan-\\nder Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes.\\nBuilding machine translation systems for the next thousand languages, 2022.\\nS´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi,\\nMarco T ´ulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experi-\\nments with GPT-4. ArXiv preprint , abs/2303.12712, 2023. URL https://arxiv.org/abs/\\n2303.12712 .\\nIsaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. Language ID in the wild: Un-\\nexpected challenges on the path to a thousand-language web text corpus. In Proceedings of the\\n28th International Conference on Computational Linguistics , pp. 6588–6608, Barcelona, Spain\\n(Online), 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.\\ncoling-main.579. URL https://aclanthology.org/2020.coling-main.579 .\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\\nlanguage models trained on code. ArXiv preprint , abs/2107.03374, 2021. URL https://\\narxiv.org/abs/2107.03374 .\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\\nSchulman. Training verifiers to solve math word problems. ArXiv preprint , abs/2110.14168,\\n2021. URL https://arxiv.org/abs/2110.14168 .\\nMarta R. Costa-juss `a, James Cross, Onur C ¸ elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffer-\\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guil-\\nlaume Wenzek, Al Youngblood, Bapi Akula, Lo ¨ıc Barrault, Gabriel Mejia Gonzalez, Prangthip\\nHansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit,\\nChau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan,\\nCynthia Gao, Vedanuj Goswami, Francisco Guzm ´an, Philipp Koehn, Alexandre Mourachko,\\nChristophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left be-\\nhind: Scaling human-centered machine translation. ArXiv preprint , abs/2207.04672, 2022. URL\\nhttps://arxiv.org/abs/2207.04672 .\\n10Siddharth Dalmia, Dmytro Okhonko, Mike Lewis, Sergey Edunov, Shinji Watanabe, Florian Metze,\\nLuke Zettlemoyer, and Abdelrahman Mohamed. LegoNN: Building Modular Encoder-Decoder\\nModels, 2022. URL https://arxiv.org/abs/2206.03318 .\\nXavier Garcia, Aditya Siddhant, Orhan Firat, and Ankur Parikh. Harnessing multilinguality in un-\\nsupervised machine translation for rare languages. In Proceedings of the 2021 Conference of\\nthe North American Chapter of the Association for Computational Linguistics: Human Lan-\\nguage Technologies , pp. 1126–1137, Online, 2021. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2021.naacl-main.89. URL https://aclanthology.org/2021.\\nnaacl-main.89 .\\nGoogle, Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H.\\nClark, Laurent El Shafey, Yanping Huang, Katlorahy Meier-Hellstern, Gaurav Mishra, Erica\\nMoreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong\\nXu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan\\nBotha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin\\nCherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl ´ement Crepy, Shachi Dave,\\nMostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D ´ıaz, Nan Du, Ethan Dyer, Vlad Feinberg,\\nFangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas\\nGonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu,\\nJeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia,\\nKathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee,\\nEric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\\nFrederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam\\nMoussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pel-\\nlat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker\\nRiley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose\\nSlone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasude-\\nvan, Kiran V odrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai\\nWu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,\\nCe Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report,\\n2023.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, An-\\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for\\nNLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th Interna-\\ntional Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,\\nUSA, volume 97 of Proceedings of Machine Learning Research , pp. 2790–2799. PMLR, 2019.\\nURL http://proceedings.mlr.press/v97/houlsby19a.html .\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth Inter-\\nnational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 .\\nOpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9 .\\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt,\\nHannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. ArXiv preprint ,\\nabs/2212.04089, 2022. URL https://arxiv.org/abs/2212.04089 .\\nSamuel Kessler, Bethan Thomas, and Salah Karout. An Adapter Based Pre-Training for Efficient\\nand Scalable Self-Supervised Speech Representation Learning, 2021. URL https://arxiv.\\norg/abs/2107.13530 .\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,\\nVinay V . Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-\\nSolo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solv-\\ning quantitative reasoning problems with language models. In NeurIPS , 2022.\\nURL http://papers.nips.cc/paper_files/paper/2022/hash/\\n18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html .\\n11Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.\\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,\\nMichele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu\\nFu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code understanding\\nand generation. ArXiv preprint , abs/2102.04664, 2021. URL https://arxiv.org/abs/\\n2102.04664 .\\nJiaqi Ma, Zhe Zhao, Jilin Chen, Ang Li, Lichan Hong, and Ed H. Chi. SNR: sub-network\\nrouting for flexible parameter sharing in multi-task learning. In The Thirty-Third AAAI Con-\\nference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Ar-\\ntificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Ad-\\nvances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February\\n1, 2019 , pp. 216–223. AAAI Press, 2019. doi: 10.1609/aaai.v33i01.3301216. URL https:\\n//doi.org/10.1609/aaai.v33i01.3301216 .\\nMichael S Matena and Colin A Raffel. Merging models with fisher-weighted averaging. Advances\\nin Neural Information Processing Systems , 35:17703–17716, 2022.\\nMohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing.\\nArXiv preprint , abs/2306.03745, 2023. URL https://arxiv.org/abs/2306.03745 .\\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\\nFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Con-\\nference of the European Chapter of the Association for Computational Linguistics: Main Volume ,\\npp. 487–503, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\\neacl-main.39. URL https://aclanthology.org/2021.eacl-main.39 .\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-\\ntext transformer. J. Mach. Learn. Res. , 21:140:1–140:67, 2020. URL http://jmlr.org/\\npapers/v21/20-074.html .\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\\nuse tools. ArXiv preprint , abs/2302.04761, 2023. URL https://arxiv.org/abs/2302.\\n04761 .\\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\\ngingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace, 2023. URL https:\\n//arxiv.org/abs/2303.17580 .\\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush V osoughi,\\nHyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Lan-\\nguage models are multilingual chain-of-thought reasoners. In The Eleventh International Confer-\\nence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net,\\n2023. URL https://openreview.net/pdf?id=fR3wGCk-IXp .\\nAditya Siddhant, Ankur Bapna, Orhan Firat, Yuan Cao, Mia Xu Chen, Isaac Caswell, and Xavier\\nGarcia. Towards the next 1000 languages in multilingual machine translation: Exploring the\\nsynergy between supervised and self-supervised learning. ArXiv preprint , abs/2201.03110, 2022.\\nURL https://arxiv.org/abs/2201.03110 .\\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen\\nPfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin,\\nSami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska,\\nBlaise Ag ¨uera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara\\nMahdavi, Joelle K. Barral, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Shekoofeh Azizi,\\nAlan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering\\nwith large language models. ArXiv preprint , abs/2305.09617, 2023. URL https://arxiv.\\norg/abs/2305.09617 .\\n12Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman\\nGarnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on\\nNeural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pp.\\n5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html .\\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao,\\nDaxin Jiang, and Ming Zhou. K-Adapter: Infusing Knowledge into Pre-Trained Models with\\nAdapters. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 , pp.\\n1405–1418, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\\nfindings-acl.121. URL https://aclanthology.org/2021.findings-acl.121 .\\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo\\nLopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith,\\nand Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models im-\\nproves accuracy without increasing inference time. In Kamalika Chaudhuri, Stefanie Jegelka,\\nLe Song, Csaba Szepesv ´ari, Gang Niu, and Sivan Sabato (eds.), International Conference on\\nMachine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA , volume 162 of\\nProceedings of Machine Learning Research , pp. 23965–23998. PMLR, 2022. URL https:\\n//proceedings.mlr.press/v162/wortsman22a.html .\\nPrateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Resolving in-\\nterference when merging models. ArXiv preprint , abs/2306.01708, 2023. URL https:\\n//arxiv.org/abs/2306.01708 .\\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker,\\nFederico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Van-\\nhoucke, and Pete Florence. Socratic Models: Composing Zero-Shot Multimodal Reasoning with\\nLanguage, 2022. URL https://arxiv.org/abs/2204.00598 .\\n13A S UPPLEMENTARY MATERIAL FOR NTL\\nA.1 FLORES-200\\nFigure 2 depicts the gains over the anchor PaLM2-S model when augmented with a model that has\\nbeen trained on DNTL. We see a positive gain through CALM for 175 of 192 languages. The highest\\ngains are seen for low-resource languages since they are the most underrepresented in the original\\nmodel. Diminishing returns with higher resource languages is seen and this trend is similar to the\\ntrend seen for mNTL\\nB.\\nLow to High Resource Languages (#languages = 192)\\nGain over Anchor Model\\nFigure 2: Gains seen by the composed model mA⊕Bover the anchor model, mB, for the complete\\nset of FLORES-200 languages. The languages are sorted from low to high-resource.\\nmAmBmA⊕B\\n(CALM)mNTL\\nB\\nmeo 7.6 28.8 34.0 33.2\\nmfa 4.0 14.0 17.6 20.4\\npcm 4.4 34.4 33.6 31.6\\nefi 3.2 14.8 18.0 14.0\\nmin 6.0 25.2 23.6 24.8\\nilo 4.8 14.8 16.8 14.0\\nady 6.4 30.0 36.4 29.2\\nmai 3.2 22.8 24.8 21.2\\nnso 6.0 8.4 8.4 9.6\\nmzn 4.8 31.6 36.4 27.6\\nbew 4.4 33.6 34.8 33.6\\nts 4.8 7.2 10.0 11.6\\ndv 2.8 11.2 14.8 13.2mAmBmA⊕B\\n(CALM)mNTL\\nB\\nbho 4.0 23.6 29.2 22.8\\ncv 6.0 17.6 16.4 20.4\\nmni 3.6 2.8 4.4 6.0\\nor 2.4 9.6 12.4 12.0\\nkri 5.6 12.4 18.8 20.0\\ntk 5.2 27.2 29.2 28.8\\ngom 4.8 22.4 25.2 22.8\\nug 6.0 23.2 29.2 26.4\\nckb 3.2 25.6 28.0 27.2\\nas 1.2 5.2 9.2 4.0\\ndoi 3.6 17.2 22.4 21.6\\ndz 4.4 0.8 0.4 0.0\\navg. 4.5 18.6 21.4 19.8\\nTable 6: Performance evaluations on the complete set of low-resource languages for GSM-8K.\\nAugmenting mAwithmBasmA⊕Bimproves performance over mBacross a majority of languages.\\nOn average, we see an improvement of 2.8%.\\n14meo mfa pcm efi min ilo ady\\nOverlap 83.17 75.54 81.28 78.35 77.90 77.80 76.21\\nDelta 1.15 1.25 1.18 1.22 1.23 1.24 1.28\\nmai nso mzn bew ts dv bho\\nOverlap 76.63 69.58 71.32 71.37 61.62 55.18 73.67\\nDelta 1.26 1.40 1.38 1.37 1.55 1.70 1.30\\ncv mni or kri tk gom ug\\nOverlap 58.52 58.94 68.03 77.18 66.06 71.21 57.66\\nDelta 1.62 1.60 1.45 1.27 1.48 1.36 1.65\\nTable 7: Quality evaluation for the LRL GSM-8K dataset across languages. We created the dataset\\nby translating the original English sentences of GSM-8K to the target language using the Google\\nTranslate API. We measure quality by back-translating the obtained examples back to English and\\nmeasuring: (i) The overlap between the back-translated and the original English sentence, and (ii)\\nThe delta change in performance when PaLM2-S is evaluated on this back-translated version of\\nGSM-8K as compared to the original version.\\nA.2 GSM-8K\\nQuality evaluation for LRL GSM-8K As described in Section 4.2, we created the GSM-8K\\ndataset (Cobbe et al., 2021) for low-resource languages by using the Google Translate API to obtain\\nsilver translations in the target language from the source English sentence in the original dataset. We\\nperform a quality evaluation of these examples by back-translating them back to English using the\\nsame translation API and defining two metrics over it:\\n(i) Overlap: The BLUE score measure between the actual example and the back-translated example,\\n(ii) Delta: The change in performance of the PaLM2-S model when evaluated on the original GSM-\\n8K set as compared to the back-translated version.\\nTable 7 shows the values for these metrics across the various languages. We see that a decently\\nhigh overlap value is seen across all languages. At the same time, the delta in performance is also\\nminimal indicating that key attributes in the GSM-8K examples are not affected by translation.\\nResults on the complete language set Table 6 shows the comparative evaluations on the complete\\nset of 25 low-resource languages for which GSM evaluations are performed. We see an improvement\\nover the anchor model mBfor20 of 25 languages. We also compare against the fully continued pre-\\ntrained version mNTL\\nBand observe that mA⊕Boutperform it for 18 of 25 languages.\\nB Q UALITATIVE ANALYSIS\\nTable 8 depicts a few qualitative examples for the code-to-text, or the code explanation task, for\\nPython. These examples depict examples for the three broader bucket of examples that we observe\\nin cases when CALM yields the correct responses:\\n1. When neither of mAormBgenerates the correct response but mA⊕Bcorrectly attends over\\ntheir latent representations to yield the correct output,\\n2. When either of mAormBis seen to give the correct response while the other one is incor-\\nrect and mA⊕Bgenerates the correct response that matches the generation from the correct\\nmodel of mAandmB, and\\n3. When both mAandmBgenerate the correct response and mA⊕Breproduces those genera-\\ntions.\\nWe also observed similar qualitative patterns with other tasks for language inclusivity.\\nC O VERHEAD WITH CALM\\nIn this section, we include a detailed computation of the expected parametric and training overhead\\nwhile composing given models using our proposed CALM framework.\\n15def ConsumeBool( self ):\\ntry :\\nresult = ParseBool( self .token)\\nexcept ValueError as e :\\nraise self .ParseError( str(e))\\nself .NextToken()\\nreturn result\\n⇒Consumes a boolean\\nmA:Consumes a boolean\\nmB:The object is not a member\\nCALM: Consumes a booleandef value( self ):\\nif self .has value:\\nreturn self .impl[OBJ]. get val(K)\\nelse :\\nraise ValueError(\"Not found\")\\nreturn\\n⇒Print an error message and exit.\\n[a part of the given model prefix]\\nExit with error message\\nPrint an error message and exit\\ndef get positions(url):\\ndata = get resource(url)\\npositions = [x for xindata[’p’]]\\nreturn positions\\n⇒Returns a list of positions.\\nPositions of specified instruments.\\nGet all positions.\\nReturns a list of positions .def distance(x0, y0, x1, y1):\\nreturn (\\nsqrt( pow(x1−x0,2) + pow(y1−y0 ,2)\\n)\\n⇒Returns the distance between two points\\nCalculates the distance between two points\\nReturn the distance between two points\\nCalculates the distance between two points\\nTable 8: Cherry-picked qualitative examples for the code-to-text task on Python that depict examples\\nthat fall into a set of larger bucket of patterns that we observe across examples. CALM does well\\nin various settings: (i) when mAproduces the correct output but not mB, (ii) vice-versa—when mB\\ndoes well, and (iii) when neither of the two base models do well but a combination of intermediate\\nrepresentations allow the composed model to give the correct output. This shows that composition\\nimplicitly learns to do both: routing across models and a combination, based on a given input.\\nC.1 P ARAMETRIC OVERHEAD\\nBuilding from the notations in §3.1, let’s say the two models mAandmBhaveNAandNBnumber\\nof standard transformer layers, respectively, with each layer of output dimensionality DAandDB.\\nAs mentioned, we choose n=nA=nBnumber of layers to perform the composition.\\n# Parameters for each fprojlayer = (DA∗DB)\\n# Parameters for each fcrosslayer = (3∗D2\\nB)\\n# Parameters added during composition =n∗(DA∗DB+ 3∗D2\\nB)\\n# Parameters in mB=NB∗(VB∗DB+ 3∗D2\\nB+ 2∗DB∗DB∗KB)\\nwhere, VBandKBdepict the vocabulary size and hidden multiplication factor, respectively.\\nLet’s consider some standard transformer configurations to understand the parameter overhead. As\\nan example, consider the layer configurations of standard BERT models: BERT-small ( mA) and\\nBERT-large ( mB). In this case: NA= 4,DA= 512, NB= 24, DB= 1024, VB= 30K, KB= 4.\\nAssuming that we select all layers of mB, the value of n= 4. Hence,\\n# Parameters added during composition = 4∗(512∗1024 + 3 ∗10242)≈1.5×107≈15M\\n# Parameters in mB= 24∗(30K∗1024 + 3 ∗10242+ 2∗10242∗4)≈1B\\n%age of new parameters added = 15 M∗100/1B= 1.5%\\nHence, number of parameters added during composition ≈1.5% of those in mB.\\nC.2 T RAINING OVERHEAD\\nWhile back propagation over mBis indeed required while training CALM, the total training costs\\nare still significantly lesser than training mB, owing to the training examples/iterations required.\\nFirstly, as discussed above, the additional number of parameters introduced during composition is\\n1.5% of the number of parameters of mB—hence, a negligible parametric addition.\\n16Further, since only 5-7% of the total mBfine-tuning data is required to train CALM, the training\\ncost of CALM is minimal with respect to training cost of training the entire anchor model.\\nMoreover, since our experiments consider an mAthat has 5-20% of parameters as mB, even the net\\ncost of training mAand CALM is significantly lesser than training mB.\\nLet’s assume that (i) the cost of fine-tuning mBon the complete data is X, (ii) number of parameters\\ninmAis 10% of those in mB, and (iii) the amount of data required to train CALM is 2% of mB\\ntraining. Assuming a linear scaling factor of training cost (FLOPS) with model parameters and data:\\nCost of training CALM ≈0.02×X= 2% of mBtraining.\\nCost of training mA+ CALM ≈(0.10∗X+ 0.02∗X) = 0.12×X= 12% of mBtraining.\\n17'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = \"/Users/user/Documents/Project/Multi_PDFs_ChatApp_AI_Agent/docs\"\n",
    "files_in_folder = os.listdir(\"docs\")\n",
    "data,chunks,ids = get_pdf_text(files_in_folder,folder_path)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGI.pdf.0',\n",
       " 'AGI.pdf.1',\n",
       " 'AGI.pdf.2',\n",
       " 'AGI.pdf.3',\n",
       " 'AGI.pdf.4',\n",
       " 'Understanding LLMs.pdf.0',\n",
       " 'Understanding LLMs.pdf.1',\n",
       " 'Understanding LLMs.pdf.2',\n",
       " 'ComputerVision.pdf.0',\n",
       " 'ComputerVision.pdf.1',\n",
       " 'AI Assistants.pdf.0',\n",
       " 'AI Assistants.pdf.1',\n",
       " 'AI Assistants.pdf.2',\n",
       " 'AI Assistants.pdf.3',\n",
       " 'Augmented LLMs.pdf.0',\n",
       " 'Augmented LLMs.pdf.1']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = get_vector_store(embeddings_nomic,chunks,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding function - <langchain_nomic.embeddings.NomicEmbeddings object at 0x127751fa0>\n",
      "Index of doc - {0: 'AGI.pdf.0', 1: 'AGI.pdf.1', 2: 'AGI.pdf.2', 3: 'AGI.pdf.3', 4: 'AGI.pdf.4', 5: 'Understanding LLMs.pdf.0', 6: 'Understanding LLMs.pdf.1', 7: 'Understanding LLMs.pdf.2', 8: 'ComputerVision.pdf.0', 9: 'ComputerVision.pdf.1', 10: 'AI Assistants.pdf.0', 11: 'AI Assistants.pdf.1', 12: 'AI Assistants.pdf.2', 13: 'AI Assistants.pdf.3', 14: 'Augmented LLMs.pdf.0', 15: 'Augmented LLMs.pdf.1'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding function - {vector_store.embedding_function}\")\n",
    "print(f\"Index of doc - {vector_store.index_to_docstore_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is Sign Language Understanding in computer vision?\n"
     ]
    }
   ],
   "source": [
    "#user_question = input(\"Ask a Question from the PDF Files uploaded - \")\n",
    "user_question = \"what is Sign Language Understanding in computer vision?\"\n",
    "print(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': 'Based on the provided text, Sign Language Understanding (SLU) in computer vision encompasses several research directions: sign language recognition (SLR), sign language translation (SLT), sign spotting, and sign language retrieval.  SLR aims to transcribe a sign video into its constituent glosses (words or phrases representing signs), categorized into isolated SLR (ISLR) for single signs and continuous SLR (CSLR) for sequences of signs. SLT goes further, translating sign languages into spoken languages.  Sign spotting focuses on locating signs within a video, while sign language retrieval involves retrieving relevant sign videos based on textual queries.'}\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(user_question)\n",
    "\n",
    "chain = get_conversational_chain()\n",
    "response = chain(\n",
    "    {\"input_documents\":docs, \"question\": user_question}\n",
    "    , return_only_outputs=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ComputerVision.pdf.1'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sign language understanding in computer vision encompasses several research areas, including sign language recognition (SLR), sign language translation (SLT), sign spotting, and sign language retrieval.  SLR aims to transcribe sign videos into their constituent glosses (words or phrases representing signs), categorized as isolated SLR (ISLR) for single signs and continuous SLR (CSLR) for sequences of signs.  SLT goes further, translating sign languages into spoken languages.  Sign spotting focuses on locating signs within videos, while sign language retrieval involves retrieving relevant sign videos based on textual queries.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_GenAI_Chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
